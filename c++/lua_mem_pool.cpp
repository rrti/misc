#include <cassert>
#include <cstdint> // std::uint8_t
#include <cstring> // std::mem{cpy,set}

#include <algorithm> // std::min
#include <deque>
#include <new>

#include <lua5.1/lua.h>
#include "lua_mem_pool.hpp"

// if 1, this places an upper limit on pool allocation size
// the pool is specifically designed to handle frequent small
// *short-lived* allocation requests in a subset of the range
// [0, MAX_ALLOC_SIZE] since these are typically generated by
// Lua VM states, not arbitrary sizes
#define CHECK_MAX_ALLOC_SIZE 1


namespace util {
	// stubs
	struct mutex {
		void lock() {}
		void unlock() {}
	};
};


static t_lua_mem_pool g_shared_pool(-1);

static std::deque<t_lua_mem_pool> g_pools;
static std::vector<size_t> g_indcs;
static util::mutex g_mutex;


static bool alloc_internal(size_t size) { return ((size * CHECK_MAX_ALLOC_SIZE) <= t_lua_mem_pool::MAX_ALLOC_SIZE); }
static bool alloc_external(size_t size) { return (!t_lua_mem_pool::is_enabled() || !alloc_internal(size)); }


void* t_lua_mem_pool::vm_alloc_func(void* ud, void* ptr, size_t osize, size_t nsize) {
	t_lua_ctx_data* lcd = static_cast<t_lua_ctx_data*>(ud);
	t_lua_mem_pool* lmp = lcd->lmp;

	assert(lmp != nullptr);

	if (nsize == 0) {
		// deallocation; must return NULL
		lmp->free(ptr, osize);
		return nullptr;
	}

	// (re)allocation; ptr is NULL if and only if osize is zero
	// * behaves like realloc when nsize!=0 and osize!=0 (ptr != NULL)
	// * behaves like malloc when nsize!=0 and osize==0 (ptr == NULL)
	return (lmp->realloc(ptr, nsize, osize));
}


t_lua_mem_pool* t_lua_mem_pool::acquire_shared_ptr() { return &g_shared_pool; }
t_lua_mem_pool* t_lua_mem_pool::acquire_ptr(const t_lua_ctx_data* ctx) {
	t_lua_mem_pool* p = acquire_shared_ptr();

	if (!ctx->share_pool) {
		// for non-shared pools, assume caller can be any thread
		g_mutex.lock();

		if (g_indcs.empty()) {
			g_pools.emplace_back(g_pools.size());
			p = &(g_pools.back());
		} else {
			p = &g_pools[g_indcs.back()];
			g_indcs.pop_back();
		}

		g_mutex.unlock();
	}

	if (!ctx->clear_pool)
		return p;

	// only wipe statistics; blocks will be reused
	// p->clear_stats((p->inc_shared_count(ctx->shared)) <= 1);

	// wipe statistics and blocks if we are the first to request p
	// if shared and count is greater than 1, do not touch the pool
	if ((p->inc_shared_count(ctx->share_pool)) <= 1) {
		p->clear();
		p->reserve(16384);
	}

	return p;
}

bool t_lua_mem_pool::release_ptr(const t_lua_ctx_data* ctx) {
	// caller decides if and when to clear
	if (ctx->lmp == acquire_shared_ptr())
		return (ctx->lmp->inc_shared_count(-1) == 0);

	g_mutex.lock();
	g_indcs.push_back(ctx->lmp->get_global_index());
	g_mutex.unlock();
	return true;
}

void t_lua_mem_pool::free_shared() { g_shared_pool.clear(); }
void t_lua_mem_pool::init_static() {
	// g_pools.reserve(16);
	g_indcs.reserve(16);
}
void t_lua_mem_pool::kill_static() {
	g_pools.clear(); // invoke dtors
	g_indcs.clear();
}



void t_lua_mem_pool::log_stats(const char* header) const {
	#if (LMP_TRACK_ALLOCS == 1)
	std::printf(
		"[lua_mem_pool::%s][%s] index=%ld {blocks,sizes}={%lu,%lu} {int,ext,rec}_allocs={%lu,%lu,%lu} {chunk,block}_bytes={%lu,%lu}\n",
		__func__,
		header,
		m_global_index,
		m_alloc_blocks.size(),
		m_chunk_count_table.size(),
		m_alloc_stats[STAT_NIA],
		m_alloc_stats[STAT_NEA],
		m_alloc_stats[STAT_NRA],
		m_alloc_stats[STAT_NCB],
		m_alloc_stats[STAT_NBB]
	);
	#endif
}


void t_lua_mem_pool::delete_blocks() {
	#if (LMP_TRACK_ALLOCS == 1)
	for (void* p: m_alloc_blocks) {
		::operator delete(p);
	}

	m_alloc_blocks.clear();
	#endif
}

void* t_lua_mem_pool::alloc(size_t size) {
	assert(size != 0);

	if (alloc_external(size)) {
		set_alloc_stat(STAT_NEA, get_alloc_stat(STAT_NEA) + 1);
		return (::operator new(size));
	}

	set_alloc_stat(STAT_NIA, get_alloc_stat(STAT_NIA) + 1);
	set_alloc_stat(STAT_NCB, get_alloc_stat(STAT_NCB) + (size = std::max(size, size_t(MIN_ALLOC_SIZE))));

	// pair<iter, bool>
	auto free_chunks_pair = std::make_pair(m_free_chunks_table.find(size), false);

	if (free_chunks_pair.first == m_free_chunks_table.end())
		free_chunks_pair = m_free_chunks_table.insert(std::make_pair(size, nullptr));

	auto& chunk_iter = free_chunks_pair.first;
	void* chunk_ptr = chunk_iter->second;

	if (chunk_ptr != nullptr) {
		// first sizeof(void*) bytes of any chunk point to the next
		// unallocated, copy them so a subsequent alloc-request (of
		// the same size) will return that chunk if non-null
		std::memcpy(&chunk_iter->second, chunk_ptr, sizeof(void*));

		// equivalent
		// chunk_iter->second = *reinterpret_cast<void**>(chunk_ptr);

		set_alloc_stat(STAT_NRA, get_alloc_stat(STAT_NRA) + 1);
		return chunk_ptr;
	}

	// pair<iter, bool>
	auto chunk_count_pair = std::make_pair(m_chunk_count_table.find(size), false);

	if (chunk_count_pair.first == m_chunk_count_table.end())
		chunk_count_pair = m_chunk_count_table.insert(std::make_pair(size, 8));

	const size_t num_chunks = (chunk_count_pair.first)->second;
	const size_t num_bytes = size * num_chunks;

	void* new_block = ::operator new(num_bytes);
	uint8_t* new_bytes = reinterpret_cast<uint8_t*>(new_block);

	#if (LMP_TRACK_ALLOCS == 1)
	m_alloc_blocks.push_back(new_block);
	#endif

	// new allocation; construct chain of chunks within the memory block
	// (this requires the block size to be at least MIN_ALLOC_SIZE bytes)
	for (size_t i = 0; i < (num_chunks - 1); ++i) {
		const void* addr_val = &new_bytes[(i + 1) * size];
		const void** addr_ptr = &addr_val;

		// write next-pointer into first bytes of chunk
		std::memcpy(&new_bytes[i * size], addr_ptr, sizeof(void*));

		// equivalent
		// *reinterpret_cast<void**>(&new_bytes[i * size]) = &new_bytes[(i + 1) * size];
	}

	std::memset(&new_bytes[(num_chunks - 1) * size], 0, sizeof(void*)); // list tail
	std::memcpy(&m_free_chunks_table[size], new_block, sizeof(void*)); // next free

	// equivalent
	// *reinterpret_cast<void**>(&new_bytes[(num_chunks - 1) * size]) = nullptr;
	// m_free_chunks_table[size] = *reinterpret_cast<void**>(new_block);

	// increase number of chunks per block geometrically for this size
	m_chunk_count_table[size] *= 2;

	set_alloc_stat(STAT_NBB, get_alloc_stat(STAT_NBB) + num_bytes);
	return new_block;
}

void* t_lua_mem_pool::realloc(void* ptr, size_t nsize, size_t osize) {
	void* ret = alloc(nsize);

	if (ptr == nullptr)
		return ret;

	assert(osize != 0);
	std::memcpy(ret, ptr, std::min(nsize, osize));
	std::memset(ptr, 0, osize);

	free(ptr, osize);
	return ret;
}

void t_lua_mem_pool::free(void* ptr, size_t size) {
	if (ptr == nullptr)
		return;

	assert(size != 0);

	if (alloc_external(size)) {
		::operator delete(ptr);
		return;
	}

	set_alloc_stat(STAT_NCB, get_alloc_stat(STAT_NCB) - (size = std::max(size, size_t(MIN_ALLOC_SIZE))));

	// write address of an available unallocated chunk (null if
	// none) into first sizeof(void*) bytes of this freed chunk
	std::memset(ptr, 0, size);
	std::memcpy(ptr, &m_free_chunks_table[size], sizeof(void*));

	// equivalent
	// *reinterpret_cast<void**>(ptr) = m_free_chunks_table[size];

	m_free_chunks_table[size] = ptr;
}



int main(int argc, char** argv) {
	{
		argv += (argc > 1);
		argc -= (argc > 1);
	}
	{
		t_lua_mem_pool::init_static();

		t_lua_ctx_data ctxs[16];

		for (int i = 0; i < 16; i++) {
			ctxs[i].share_pool = (i & 1);
			ctxs[i].clear_pool = true;

			ctxs[i].lmp = t_lua_mem_pool::acquire_ptr(&ctxs[i]);
			ctxs[i].lvm = lua_newstate(&t_lua_mem_pool::vm_alloc_func, &ctxs[i]);
		}

		for (int i = 0; i < 16; i++) {
			lua_close(ctxs[i].lvm);

			if (!t_lua_mem_pool::release_ptr(&ctxs[i]))
				continue;

			ctxs[i].lmp->log_stats(__func__);
			ctxs[i].lmp->clear();
		}

		t_lua_mem_pool::kill_static();
	}

	return 0;
}

